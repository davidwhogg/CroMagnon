%% This file is part of the CroMagnon project
%% Copyright 2015 David W. Hogg

\documentclass[12pt]{article}
\usepackage{url}
\input{vc}

\newcommand{\like}{L}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\inverse}{^{-1}}
\newcommand{\transpose}{^{\mathsf{T}}}

\begin{document}

\section*{Notes on log-sum-exp%
\footnote{\raggedright This document is available (in source-code form) at \giturl.
This precise version has git hash \texttt{\githash~(\gitdate)}.}}
\noindent
David W. Hogg (NYU) (SCDA) (MPIA)

\bigskip

Generically in probabilistic inference problems we encounter quantities like
\begin{eqnarray}
  Q
  &=& \ln\sum_{k=1}^K\exp q_k
  \quad,
\end{eqnarray}
where the $q_k$ are simple functions (for example, they might have simple
derivatives with respect to parameters $\theta$).
Let's take the full derivative of $Q$:
\begin{eqnarray}
  \frac{\dd Q}{\dd\theta}
  &=& \left[\sum_{k=1}^K\exp q_k\right]\inverse
      \,\frac{\dd}{\dd\theta}\left[\sum_{k=1}^K\exp q_k\right] \\
  &=& \left[\sum_{k=1}^K\exp q_k\right]\inverse
      \,\left[\sum_{k=1}^K\frac{\dd q_k}{\dd\theta}\exp q_k\right]
  \quad;
\end{eqnarray}
that is, the derivative of $Q$ is just the $\exp q_k$-weighted average
of derivatives of the $q_k$.
That's good!

In the particular case of cryo-EM, the objective function is related
to a marginalized likelihood $\like$ which looks like
\begin{eqnarray}
  Q
  &=&
  \sum_{n=1}^N Q_n \\
  Q_n
  &=& -2\,\ln\like_n \\
  &=& -2\,\ln\sum_{k=1}^K P_k\,\exp(\ln\like_{nk}) \\
  &=& -2\,\ln\sum_{k=1}^K P_k\,\exp(-\frac{1}{2}\,\chi^2_{nk}) \\
  \chi^2_{nk}
  &=& [y_n - \mu_{nk}]\transpose\cdot C_n\inverse\cdot [y_n - \mu_{nk}]
  \quad,
\end{eqnarray}
where the sum is over $k$ samples or grid locations in angles and
offets, the $P_k$ are weights for those samples (typically roughly
$1/K$), $y_n$ is the $n$th data point (tiny image), the $\mu_{nk}$ are
the predictions for data point $n$ at angles and offsets $k$, and
$C_n$ is a covariance matrix for the (Gaussian) noise model.
The predictions $\mu_{nk}$ depend on the three-dimensional shape
parameters or representation $\theta$.

Taking derivatives with respect to parameters,
\begin{eqnarray}
  \frac{\dd Q}{\dd\theta}
  &=&
  \sum_{n=1}^N \frac{\dd Q_n}{\dd\theta} \\
  \frac{\dd Q_n}{\dd\theta}
  &=& -2\,\left[\sum_{k=1}^K P_k\,\exp(-\frac{1}{2}\,\chi^2_{nk})\right]\inverse
        \,\left[\sum_{k=1}^K -\frac{1}{2}\,P_k\,\exp(-\frac{1}{2}\,\chi^2_{nk})\,\frac{\dd\chi^2_{nk}}{\dd\theta}\right] \\
  \frac{\dd\chi^2_{nk}}{\dd\theta}
  &=& -2\,[y_n - \mu_{nk}]\transpose\cdot C_n\inverse\cdot\frac{\dd\mu_{nk}}{\dd\theta}
  \quad.
\end{eqnarray}

There is a literature on using the expectation-maximization (E-M)
algorithm to optimize marginalized likelihoods that have the logsumexp
form.
A good pedagogical (but not original) reference is Bishop (2006; ch
9).
The E-M algorithm is clever because it optimizes a marginalized
likelihood of the form of the $Q$ defined above, without ever
explicitly constructing the marginalized likelihood or its
derivatives.
And the proof is beautiful!
Rumors fly, however, that E-M is not the best optimizer for many
real problems (Salakhutdinov 2003a, 2003b), and that if we are willing
to compute the derivatives, we might get to the local optimum faster,
even given the extra cost per iteration.

\begin{trivlist}\raggedright
\item
Bishop,~C., 2006,
\textit{Pattern Recognition and Machine Learning},
Springer-Verlag, New York
{\footnotesize ISBN:978-0-387-31073-2}.
\item
Salakhutdinov,~R., Roweis,~S., \& Ghahramani,~Z., 2003a,
``Optimization with EM and Expectation-Conjugate-Gradient'',
\textit{Proc.~20~ICML},
{\footnotesize \url{http://www.cs.nyu.edu/~roweis/papers/emecgicml03.pdf}}.
\item
Salakhutdinov,~R., Roweis,~S., \& Ghahramani,~Z., 2003b,
``On the Convergence of Bound Optimization Algorithms'',
\textit{Proc.~19~UAI (UAI2003)},
{\footnotesize \url{http://arxiv.org/abs/1212.2490}}.
\end{trivlist}

\end{document}
